# -*- coding: utf-8 -*-
"""full_finetuning

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1exbpuMR6p15AlAgvTq_dgu_9UcNivehO

# Full Finetuning
This a script that performs full finetuning of different multilingual PLMs on our Warao-Spanish parallel data.

It leverages the ML HuggingFace classes for training a sequence model and tokenizing.

This code is adapted from the https://github.com/masakhane-io/lafand-mt/tree/main.

**Citation**
> Adelani, D., Alabi, J., Fan, A., Kreutzer, J., Shen, X., Reid, M., Ruiter, D., Klakow, D., Nabende, P., Chang, E., Gwadabe, T., Sackey, F., Dossou, B. F. P., Emezue, C., Leong, C., Beukman, M., Muhammad, S., Jarso, G., Yousuf, O., Niyongabo Rubungo, A., … Manthalu, S. (2022). A few thousand translations go a long way! Leveraging pre-trained models for African news translation. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (pp. 3053–3070). Association for Computational Linguistics. https://doi.org/10.18653/v1/2022.naacl-main.223
"""

# !pip install sacrebleu==2.0.0
# !pip install protobuf
# !pip show sentencepiece
# !pip install peft accelerate
# !pip install evaluate

model_configs = {
    "facebook/m2m100_418M": ("pt", "es"),   # order: src, target
    "facebook/m2m100_1.2B": ("pt", "es"),
    "facebook/mbart-large-50": ("pt_XX", "es_XX"),
    "facebook/mbart-large-50-many-to-many-mmt": ("pt_XX", "es_XX"),
    # "facebook/mbart-large-50-one-to-many-mmt": ("pt_XX", "es_XX"),
    "facebook/nllb-200-distilled-600M": ("wro_Latn", "spa_Latn"),  # fake source code for this model seems to work best
    # "google/mt5-base": ("Warao", "Spanish"),
    "google/mt5-small": ("Warao", "Spanish"),
    # "google/byt5-base": ("Warao", "Spanish"),
    "google/byt5-small": ("Warao", "Spanish"),

}

# hyperparameter tuning
HP_TUNE_PROJECT_NAME = "warao_spanish_mt"
RUN_SWEEP = True
HP_COUNT = 5  # number of times agent will run wandb sweep

MODEL_NAME = list(model_configs.keys())[5]
MODEL = MODEL_NAME.split('/')[1]
do_predictions=False if RUN_SWEEP else True  # dont output predictions when doing HP tuning
do_pred_on_test=False
do_evaluate=False if RUN_SWEEP else True # no point in runnning eval on val set during HP tuning -- this is done anyway after last epoch
do_eval_on_test=False # if False, does eval on val set


TRAIN_FILE = "final_parallel_train.csv"
VAL_FILE = "final_parallel_val.csv"
TEST_FILE = "final_parallel_test.csv"
OUTPUT_DIR = f"./{MODEL}-finetuned-warao-es"
SOURCE_CODE = model_configs[MODEL_NAME][0]
TARGET_CODE = model_configs[MODEL_NAME][1]
MAX_LEN = 128
EPOCHS = 3
BATCH_SIZE = 16
LEARNING_RATE = 1e-4
NUM_BEAMS = 4
WANDB_RUN_NAME = f"{MODEL}_full_ft"
WANDB_PROJECT = "huggingface" # project name for normal full fts
LOG_STEPS = 1 if TRAIN_FILE == "toy_data.csv" else 100
WARMUP_STEPS = 0



sweep_config = {
    'method': 'bayes',
    'metric': {
        'name': 'eval/bleu',
        'goal': 'maximize'
    },
    'parameters': {
        'learning_rate': {
            'distribution': 'log_uniform_values',
            # 'min': 1e-5,  # for non-t5 models
            # 'max': 5e-4
            'min': 1e-4, # for t5 models
            'max': 5e-3
        },
        'batch_size': {
            'values': [8, 16]
        },
        'num_train_epochs': {
            # 'values': [3, 5] # for non-t5 models
            'values': [5, 7] # for t5 models
            
        },
        'weight_decay': {
            'distribution': 'uniform',
            'min': 0.0,
            'max': 0.1
        },
        # 'max_length': {
        #     'values': [64, 128, 256]
        # }
    }
}

import os
import sys
import logging
import numpy as np
from datasets import load_dataset
from evaluate import load
from transformers import (
    # AutoConfig,
    AutoModelForSeq2SeqLM,
    AutoTokenizer,
    DataCollatorForSeq2Seq,
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments,
    set_seed,
)
import torch
# import random
import pandas as pd
import wandb
from wandb import Table
import gc
import os
if "nllb" in MODEL_NAME or "mbart" in MODEL_NAME:
    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'


# FBTs depend on the model type
def get_forced_bos_token_id(model_name, tokenizer, target_lang):
    if "m2m100" in model_name:
        return tokenizer.get_lang_id(target_lang)
    elif "mbart" in model_name:
        return tokenizer.lang_code_to_id[target_lang]
    elif "nllb" in model_name:
        return tokenizer.convert_tokens_to_ids(target_lang)
    else:
        # T5 models don't have forced_bos_token_id -- make this default behavior
        return None


def start_training(model_name_or_path, train_file, val_file, test_file, output_dir,
                  source_lang=None, target_lang=None,
                  max_source_length=400, max_target_length=400,
                  num_train_epochs=3, batch_size=8, learning_rate=1e-5, num_beams=4,
                  do_predictions=True, do_pred_on_test=False,
                  do_evaluate=True, do_eval_on_test=False, wandb_project="full_ft",
                  wandb_run_name=None, weight_decay=0.01, warmup_steps=0, use_wandb=False,
                  logging_steps=100):

    # set up logging
    logging.basicConfig(
        format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
        datefmt="%m/%d/%Y %H:%M:%S",
        handlers=[logging.StreamHandler(sys.stdout)],
    )
    logger = logging.getLogger(__name__)
    logger.setLevel(logging.INFO)

    # load datasets
    print("\n" + "=" * 50)
    logger.info("Loading datasets . . . ")
    print("=" * 50)
    if do_eval_on_test:
      data_files = {"train": train_file, "validation": val_file, "test": test_file}
    else:
      data_files = {"train": train_file, "validation": val_file}
    raw_datasets = load_dataset("csv", data_files=data_files)

    # preview datasets
    print("\n" + "=" * 50)
    print("Previewing datasets . . . ")
    print("=" * 50)
    for split, dataset in raw_datasets.items():
        print(f"{split}: {dataset[:5]}")


    # check GPU mem 
    def print_gpu_memory():
        if torch.cuda.is_available():
            print(f"Allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB")
            print(f"Reserved: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB")
            print(f"Max allocated: {torch.cuda.max_memory_allocated(0) / 1024**3:.2f} GB")

    # call @ diff pts to check mem 
    print_gpu_memory() 

    # load model + tokenizer
    print("\n" + "=" * 50)
    logger.info(f"Loading {model_name_or_path} model and tokenizer with source code {source_lang} and target code {target_lang} . . . ")
    print("=" * 50)
    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path)
    print_gpu_memory() 
    model = AutoModelForSeq2SeqLM.from_pretrained(
                                                    model_name_or_path, 
                                                    # dtype=torch.float16 if "nllb" or "mbart" in model_name_or_path else torch.float32
                                                )
    print_gpu_memory() 
    

    # check if model is a T5-based model
    is_t5_model = "mt5" in model_name_or_path or "byt5" in model_name_or_path

    # T5-based models dont have language codes
    if not is_t5_model:
        tokenizer.src_lang = source_lang
        tokenizer.tgt_lang = target_lang

        forced_bos_token_id = get_forced_bos_token_id(model_name_or_path, tokenizer, target_lang)
        if forced_bos_token_id is not None:
            model.config.forced_bos_token_id = forced_bos_token_id
    else:
        if model.config.decoder_start_token_id is None:
            model.config.decoder_start_token_id = tokenizer.pad_token_id

    # preprocess raw data -- tokenize + add prefix if necessary
    def preprocess_function(examples):
        inputs = examples["warao_sentence"]
        targets = examples["spanish_sentence"]

        # T5 models need task prefix
        if is_t5_model:
            inputs = [f"translate {source_lang} to {target_lang}: {text}" for text in inputs]

        model_inputs = tokenizer(inputs, max_length=max_source_length, truncation=True, padding=False)
        labels = tokenizer(targets, max_length=max_target_length, truncation=True, padding=False)
        model_inputs["labels"] = labels["input_ids"]
        return model_inputs

    tokenized_datasets = raw_datasets.map(preprocess_function, batched=True)

    data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)

    bleu_metric = load("sacrebleu")
    chrf_metric = load("chrf")

    def postprocess_text(preds, labels):
        preds = [p.strip() for p in preds]
        labels = [[l.strip()] for l in labels]
        return preds, labels

    def compute_metrics(eval_preds):  # adapted from lafand.md
        preds, labels = eval_preds
        if isinstance(preds, tuple):
            preds = preds[0]
        decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)

        # Replace -100 in the labels as we can't decode them
        labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
        decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

        # Post-processing
        decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)

        bleu_res = bleu_metric.compute(predictions=decoded_preds, references=decoded_labels)
        result = {"bleu": bleu_res["score"]}

        result["chrf"] = chrf_metric.compute(predictions=decoded_preds, references=decoded_labels)["score"]
        prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]
        result["gen_len"] = np.mean(prediction_lens)
        result = {k: round(v, 4) for k, v in result.items()}
        return result
    
    # if is_t5_model and batch_size > 8:
    #     # Use gradient accumulation for larger effective batch sizes
    #     gradient_accumulation_steps = batch_size // 8
    #     effective_batch_size = 8
    #     logger.info(f"Using gradient accumulation: {gradient_accumulation_steps} steps for effective batch size {batch_size}")


    training_args = Seq2SeqTrainingArguments(
        output_dir=output_dir,
        eval_strategy="epoch",
        save_strategy="no",
        learning_rate=learning_rate,
        per_device_train_batch_size=batch_size,
        per_device_eval_batch_size=batch_size,
        weight_decay=weight_decay,
        save_total_limit=1,
        num_train_epochs=num_train_epochs,
        predict_with_generate=True,
        generation_max_length=max_target_length,
        generation_num_beams=num_beams,
        logging_dir=os.path.join(output_dir, "logs"),
        logging_steps=logging_steps,
        report_to="wandb" if use_wandb else "none",
        run_name=wandb_run_name,
        load_best_model_at_end=False,
        metric_for_best_model="bleu",
        greater_is_better=True,
        gradient_checkpointing=True if "nllb" in model_name_or_path or "mbart" in model_name_or_path else False, # trade comp time for memory
        fp16=True if "nllb" in model_name_or_path or "mbart" in model_name_or_path else False, # use mixed precision for large models
    )

    trainer = Seq2SeqTrainer(
        model=model,
        args=training_args,
        train_dataset=tokenized_datasets["train"],
        eval_dataset=tokenized_datasets["validation"],
        tokenizer=tokenizer,
        data_collator=data_collator,
        compute_metrics=compute_metrics,
    )

    logger.info(f"Training . . .")
    trainer.train()

    trainer.save_model()
    logger.info("Fully Finetuned model saved!")

    # run evaluation
    if do_evaluate:
        logger.info(f"*** Doing Evaluations on {'test' if do_eval_on_test else 'validation'} set ***")
        if do_eval_on_test:
            eval_dataset = tokenized_datasets["test"]
        else:
            eval_dataset = tokenized_datasets["validation"]
        eval_results = trainer.evaluate(eval_dataset)
        logger.info(f"BLEU on validation set: {eval_results}")

    # generate predictions
    def do_predictions_func(tok_dataset, reg_dataset, trainer, auto_tokenizer):
        logger.info("*** Generating Predictions ***")

        predict_results = trainer.predict(
            tok_dataset,
            metric_key_prefix="predict",
            num_beams=num_beams,
        )
        preds = auto_tokenizer.batch_decode(predict_results.predictions, skip_special_tokens=True)
        preds = [p.strip().replace("\n", " ") for p in preds]

        logger.info(f"Metrics: {predict_results.metrics}")

        pred_output_file = f'{MODEL}_ft_predictions.csv'
        df = pd.DataFrame(reg_dataset)
        df['predictions'] = preds

        os.makedirs(output_dir, exist_ok=True)
        pred_file = os.path.join(output_dir, pred_output_file)
        df.to_csv(pred_file, index=False)
        logger.info(f"*** Predictions on {'test' if do_eval_on_test else 'validation'} set saved to {pred_file} ***")
        return preds, predict_results[1:]

    if do_predictions:
        if do_pred_on_test:
            tok_pred_dataset = tokenized_datasets["test"]
            reg_pred_dataset = raw_datasets["test"]
        else:
            tok_pred_dataset = tokenized_datasets["validation"]
            reg_pred_dataset = raw_datasets["validation"]

        do_predictions_func(
            tok_dataset=tok_pred_dataset,
            reg_dataset=reg_pred_dataset,
            trainer=trainer,
            auto_tokenizer=tokenizer,
        )



    # free up memory
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

    return trainer


# perform wandb sweep for hyperparameter tuning
def sweep_train():
    # Clear at start
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.reset_peak_memory_stats()

            
    wandb.init()

    config = wandb.config # HPs stored here

    sweep_output_dir = f"{OUTPUT_DIR}_sweep_{wandb.run.id}"

    try:
        trainer = start_training(
            model_name_or_path=MODEL_NAME,
            train_file=TRAIN_FILE,
            val_file=VAL_FILE,
            test_file=TEST_FILE,
            output_dir=sweep_output_dir,
            source_lang=SOURCE_CODE,
            target_lang=TARGET_CODE,
            max_source_length=MAX_LEN,
            max_target_length=MAX_LEN,
            num_train_epochs=config.num_train_epochs,
            batch_size=config.batch_size,
            learning_rate=config.learning_rate,
            num_beams=NUM_BEAMS,
            weight_decay=config.weight_decay,
            warmup_steps=WARMUP_STEPS,
            do_predictions=do_predictions,
            do_pred_on_test=do_pred_on_test,
            do_evaluate=do_evaluate,
            do_eval_on_test=do_eval_on_test,
            wandb_project="warao_spanish_mt",
            wandb_run_name=f"{MODEL}_sweep",
            use_wandb=True,
            logging_steps=LOG_STEPS,
        )
        # delete model and trainer to free memory
        del trainer
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    except Exception as e:
        print(f"Error in sweep run: {e}") 
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    finally:  # cleanup even when we have error
        gc.collect()
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.reset_peak_memory_stats()


device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
print("USING DEVICE:", device)

wandb.login()

if RUN_SWEEP: # for hyperparameter tuning
  sweep_id = wandb.sweep(sweep_config, project=HP_TUNE_PROJECT_NAME)
  wandb.agent(sweep_id, function=sweep_train, count=HP_COUNT)
else:
  start_training(
      model_name_or_path=MODEL_NAME,
      train_file=TRAIN_FILE,
      val_file=VAL_FILE,
      test_file=TEST_FILE,
      output_dir=OUTPUT_DIR,
      source_lang=SOURCE_CODE,
      target_lang=TARGET_CODE,
      max_source_length=MAX_LEN,
      max_target_length=MAX_LEN,
      num_train_epochs=EPOCHS,
      batch_size=BATCH_SIZE,
      learning_rate=LEARNING_RATE,
      num_beams=NUM_BEAMS,
      weight_decay=0.01,
      warmup_steps=WARMUP_STEPS,
      do_predictions=do_predictions,
      do_pred_on_test=do_pred_on_test,
      do_evaluate=do_evaluate,
      do_eval_on_test=do_eval_on_test,
      wandb_project="huggingface",
      wandb_run_name=WANDB_RUN_NAME,
      use_wandb=True,
      logging_steps=LOG_STEPS,
  )

wandb.finish()
