# # -*- coding: utf-8 -*-
# """benchmarking_final.ipynb

# Automatically generated by Colab.

# Original file is located at
#     https://colab.research.google.com/drive/1zxgedXKVUjSKAMoiI2RSuGIwbFYA1i_a
# """

from evaluate import evaluator, load
from evaluate.visualization import radar_plot
import torch
import pandas as pd
from datasets import load_dataset, Dataset
from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM
from transformers.pipelines.pt_utils import KeyDataset
import logging
import sys
from tqdm.auto import tqdm
import time
import numpy as np

# # !pip install sacrebleu==2.0.0
# # !pip install protobuf
# # !pip show sentencepiece
# # !pip install peft accelerate
# # !pip install evaluate

TEST_FILE = "parallel_test.csv"  # only need test set since we'll compare performance on this set across models
BATCH_SIZE = 32
MAX_LEN = 128
NUM_BEAMS = 5

model_configs = {
    "facebook/m2m100_418M": ("pt", "es"),   # order: src, target
    # "facebook/m2m100_1.2B": ("pt", "es"),
    "facebook/mbart-large-50": ("pt_XX", "es_XX"),
    # "facebook/mbart-large-50-many-to-many-mmt": ("pt_XX", "es_XX"),
    "facebook/mbart-large-50-one-to-many-mmt": ("pt_XX", "es_XX"),
    "facebook/nllb-200-distilled-600M": ("wro_Latn", "spa_Latn"),  # fake source code for this model seems to work best
    "google/mt5-base": ("Portuguese", "Spanish"),
    "google/mt5-small": ("Portuguese", "Spanish"),
    "google/byt5-base": ("Portuguese", "Spanish"),
    "google/byt5-small": ("Portuguese", "Spanish"),

}

# models for future but were too large
    # "facebook/nllb-200-distilled-1.3B": ("wro_Latn", "spa_Latn"),
    # "google/mt5-large": ("Warao", "Spanish"),
    # "google/mt5-xl": ("Warao", "Spanish"),
    # "google/mt5-xxl": ("Warao", "Spanish"),
    # "google/byt5-large": ("Warao", "Spanish"),
    # "google/byt5-xl": ("Warao", "Spanish"),
    # "google/byt5-xxl": ("Warao", "Spanish"),
    # "facebook/nllb-200-1.3B": ("war_Latn", "spa_Latn"), # same size as 1.3B distilled but distilled has better performance
    # "facebook/nllb-200-3.3B": ("war_Latn", "spa_Latn"),

# log setup
logging.basicConfig(
    format="%(asctime)s - %(levelname)s - %(name)s - %(message)s",
    datefmt="%m/%d/%Y %H:%M:%S",
    handlers=[logging.StreamHandler(sys.stdout)],
)
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# device: are ou using GPU or CPU?
device = "cuda" if torch.cuda.is_available() else "cpu"
logger.info(f"Using device: {device}")

# load data
logger.info("Loading dataset...")
data_files = {"test": TEST_FILE}
dataset = load_dataset("csv", data_files=data_files)['test']

bleu_metric = load("sacrebleu")
chrf_metric = load("chrf")

source_sentences = [ex['warao_sentence'] for ex in dataset]
references = [ex['spanish_sentence'] for ex in dataset]
# source_sentences = ["Jebu jakutai sinarijatuma nonaya. Narune raina nomeira ekia ina airamotuma orikware toaya orikuba saba. Dio Taerajawitu jakutaisi aya urirajasi eku orikuba jate."]  
# references = ["Pues son espíritus de demonios que hacen señales, los cuales van a los reyes de todo el mundo, a reunirlos para la batalla del gran día del Dios Todopoderoso."]
 
all_results = []

def prepare_batch(batch_texts, tokenizer, src_lang, tgt_lang, model_name, max_length=128):

    if "m2m100" in model_name:
        tokenizer.src_lang = src_lang
        inputs = tokenizer(batch_texts, return_tensors="pt", padding=True,
                          truncation=True, max_length=max_length).to(device)
        forced_bos_token_id = tokenizer.get_lang_id(tgt_lang)

    elif "mbart" in model_name:
        tokenizer.src_lang = src_lang
        inputs = tokenizer(batch_texts, return_tensors="pt", padding=True,
                          truncation=True, max_length=max_length).to(device)
        forced_bos_token_id = tokenizer.lang_code_to_id[tgt_lang]

    elif "nllb" in model_name:
        tokenizer.src_lang = src_lang
        inputs = tokenizer(batch_texts, return_tensors="pt", padding=True,
                          truncation=True, max_length=max_length).to(device)
        forced_bos_token_id = tokenizer.convert_tokens_to_ids(tgt_lang)

    elif "mt5" in model_name or "byt5" in model_name:
        # T5 models use task prefix instead of language codes
        batch_with_prefix = [f"translate {src_lang} to {tgt_lang}: {text}"
                            for text in batch_texts]
        inputs = tokenizer(batch_with_prefix, return_tensors="pt", padding=True,
                          truncation=True, max_length=max_length).to(device)
        forced_bos_token_id = None

    else:
        raise ValueError(f"Unknown model type: {model_name}")

    return inputs, forced_bos_token_id

def batch_data(data, batch_size):
    for i in range(0, len(data), batch_size):
        yield data[i:i + batch_size]

def benchmark_speed(model, tokenizer, sentences, src_lang, tgt_lang, model_name,
                   batch_size, max_length, num_beams, epochs=3):

    model.eval()
    times = []
    test_batch = sentences[:batch_size]

    # warmup
    with torch.no_grad():
        inputs, forced_bos_token_id = prepare_batch(
            test_batch, tokenizer, src_lang, tgt_lang, model_name, max_length
        )
        if forced_bos_token_id is not None:
            _ = model.generate(**inputs, forced_bos_token_id=forced_bos_token_id,
                             max_length=max_length, num_beams=num_beams)
        else:
            _ = model.generate(**inputs, max_length=max_length, num_beams=num_beams)

    for _ in range(epochs):
        with torch.no_grad():
            inputs, forced_bos_token_id = prepare_batch(
                test_batch, tokenizer, src_lang, tgt_lang, model_name, max_length
            )

            start = time.time()
            if forced_bos_token_id is not None:
                _ = model.generate(**inputs, forced_bos_token_id=forced_bos_token_id,
                                 max_length=max_length, num_beams=num_beams)
            else:
                _ = model.generate(**inputs, max_length=max_length, num_beams=num_beams)

            if device == "cuda":
                torch.cuda.synchronize()

            end = time.time()
            times.append(end - start)

    avg_time = np.mean(times)
    std_time = np.std(times)
    throughput = batch_size / avg_time

    return {
        "avg_time_per_batch": avg_time,
        "std_time": std_time,  # standard deviation
        "throughput": throughput
    }

def translate_full_dataset(model, tokenizer, sentences, src_lang, tgt_lang,
                          model_name, batch_size, max_length, num_beams):

    model.eval()
    all_predictions = []

    with torch.no_grad():
        for batch in tqdm(batch_data(sentences, batch_size),
                         total=(len(sentences) + batch_size - 1) // batch_size,
                         desc="Translating"):

            # tokenize batch
            inputs, forced_bos_token_id = prepare_batch(
                batch, tokenizer, src_lang, tgt_lang, model_name, max_length
            )

            # generate translations
            if forced_bos_token_id is not None:
                outputs = model.generate(
                    **inputs,
                    forced_bos_token_id=forced_bos_token_id,
                    max_length=max_length,
                    num_beams=num_beams
                )
            else:
                outputs = model.generate(
                    **inputs,
                    max_length=max_length,
                    num_beams=num_beams
                )

            # decode
            predictions = tokenizer.batch_decode(outputs, skip_special_tokens=True)
            all_predictions.extend(predictions)

    return all_predictions

# main benchmark loop
for model_name in model_configs.keys():
    src_lang, tgt_lang = model_configs[model_name]

    logger.info(f"\n{'='*80}")
    logger.info(f"Testing model: {model_name}")
    logger.info(f"Source: {src_lang} -> Target: {tgt_lang}")
    logger.info(f"{'='*80}\n")

    try:
        # clear GPU memory
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.reset_peak_memory_stats()

        logger.info("Loading model and tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)
        model.eval()

        logger.info("Benchmarking inference speed...")
        speed_metrics = benchmark_speed(
            model, tokenizer, source_sentences, src_lang, tgt_lang,
            model_name, BATCH_SIZE, MAX_LEN, NUM_BEAMS, epochs=3
        )

        logger.info("Running full translation...")
        total_time_start = time.time()

        predictions = translate_full_dataset(
            model, tokenizer, source_sentences, src_lang, tgt_lang,
            model_name, BATCH_SIZE, MAX_LEN, NUM_BEAMS
        )

        if device == "cuda":
            torch.cuda.synchronize()

        total_time = time.time() - total_time_start

        logger.info("Calculating BLEU and chrF scores...")
        bleu_score = bleu_metric.compute(predictions=predictions, references=references)['score']
        chrf_score = chrf_metric.compute(predictions=predictions, references=references)['score']

        max_memory_gb = 0
        if torch.cuda.is_available():
            max_memory_gb = torch.cuda.max_memory_allocated() / 1024**3

        # store results
        result = {
            "model": model_name,
            "src_lang": src_lang,
            "tgt_lang": tgt_lang,
            "bleu": bleu_score,
            "chrf": chrf_score,
            "avg_time_per_batch": speed_metrics["avg_time_per_batch"],
            "std_time": speed_metrics["std_time"],
            "throughput": speed_metrics["throughput"],
            "total_translation_time": total_time,
            "samples_per_sec": len(source_sentences) / total_time,
            "max_memory_gb": max_memory_gb,
            "num_samples": len(source_sentences),
            "batch_size": BATCH_SIZE,
            "num_beams": NUM_BEAMS
        }

        all_results.append(result)

        # Log results
        logger.info(f"\n--- Results for {model_name} ---")
        logger.info(f"BLEU: {bleu_score:.2f}")
        logger.info(f"chrF: {chrf_score:.2f}")
        logger.info(f"Throughput: {speed_metrics['throughput']:.2f} samples/sec (±{speed_metrics['std_time']:.3f}s)")
        logger.info(f"Total time: {total_time:.2f}s")
        logger.info(f"Overall speed: {len(source_sentences) / total_time:.2f} samples/sec")
        logger.info(f"Max GPU memory: {max_memory_gb:.2f} GB")

        logger.info("\nSample translations:")
        for i in range(min(3, len(predictions))):
            logger.info(f"  Source: {source_sentences[i]}")
            logger.info(f"  Prediction: {predictions[i]}")
            logger.info(f"  Reference: {references[i]}\n")

        pred_filename = f"{model_name.split('/')[1]}-predictions.csv"
        df_preds = pd.DataFrame({
            'warao_sentence': source_sentences,
            'spanish_model_translation': predictions,
            'spanish_real_translation': references
        })
        df_preds.to_csv(pred_filename, index=False)
        logger.info(f"Predictions saved to: {pred_filename}")
        
        df_results = pd.DataFrame(all_results)
        df_results.to_csv("benchmark_results.csv", index=False)
        logger.info(f"Results saved to: benchmark_results.csv\n")

        # clear memory to make space for subsequent runs
        del model, tokenizer
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        logger.info(f"Cache cleared and model + tokenizer removed to make space for next benchmark!\n")

    except Exception as e:
        logger.error(f"Error with {model_name}: {e}", exc_info=True)
        continue

# generate results summary
logger.info(f"\n{'='*80}")
logger.info("FINAL BENCHMARK SUMMARY")
logger.info(f"{'='*80}\n")

df_final = pd.DataFrame(all_results)
df_final = df_final.sort_values("bleu", ascending=False)

summary_cols = ["model", "bleu", "chrf", "throughput", "samples_per_sec",
                "total_translation_time", "max_memory_gb"]
logger.info("\n" + df_final[summary_cols].to_string())

# save final results
df_final.to_csv("final_benchmark_results.csv", index=False)
logger.info(f"\nFinal results saved to: final_benchmark_results.csv")

# print top 3 models by BLEU
logger.info("\n" + "="*80)
logger.info("TOP 3 MODELS BY BLEU SCORE:")
logger.info("="*80)
for idx, row in df_final.head(3).iterrows():
    logger.info(f"\n{row['model']}")
    logger.info(f"  BLEU: {row['bleu']:.2f}")
    logger.info(f"  chrF: {row['chrf']:.2f}")
    logger.info(f"  Speed: {row['samples_per_sec']:.2f} samples/sec")
    logger.info(f"  Memory: {row['max_memory_gb']:.2f} GB")










