# -*- coding: utf-8 -*-
"""final_continual_pretraining.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12Y9Lbcn14F7ESvjyO0kYujEJdWai6Z5e

From this video: https://www.youtube.com/watch?v=PBb1FLK1VJo
"""

# !pip install sacrebleu==2.0.0
# !pip install protobuf
# !pip show sentencepiece
# !pip install peft accelerate
# !pip install evaluate

from pprint import pprint
import math
import wandb

from datasets import load_dataset, load_from_disk
from transformers import (
    AutoTokenizer,
    AutoModelForSeq2SeqLM,
    DataCollatorForSeq2Seq,
    Seq2SeqTrainer,
    Seq2SeqTrainingArguments
)
import torch
from peft import LoraConfig, get_peft_model

model_configs = {
    "facebook/m2m100_418M": ("pt", "es"),   # order: src, target
    "facebook/m2m100_1.2B": ("pt", "es"),
    "facebook/mbart-large-50": ("pt_XX", "es_XX"),
    "facebook/mbart-large-50-many-to-many-mmt": ("pt_XX", "es_XX"),
    # "facebook/mbart-large-50-one-to-many-mmt": ("pt_XX", "es_XX"),
    "facebook/nllb-200-distilled-600M": ("wro_Latn", "spa_Latn"),  # fake source code for this model seems to work best
    # "google/mt5-base": ("Warao", "Spanish"),
    "google/mt5-small": ("Warao", "Spanish"),
    # "google/byt5-base": ("Warao", "Spanish"),
    "google/byt5-small": ("Warao", "Spanish"),

}

MODEL_NAME = list(model_configs.keys())[2]
MODEL = MODEL_NAME.split('/')[1]

TRAIN_FILE = "./input/tiny_monolingual_warao_bible.txt"
VAL_FILE = "./input/toy_data.csv"
TEST_FILE = "./input/toy_data.csv"
OUTPUT_DIR = f"./{MODEL}-finetuned-warao-es"
SOURCE_CODE = model_configs[MODEL_NAME][0]
TARGET_CODE = model_configs[MODEL_NAME][1]
MAX_LEN = 128
EPOCHS = 3
BATCH_SIZE = 16
LEARNING_RATE = 1e-4
NUM_BEAMS = 4
WANDB_RUN_NAME = f"{MODEL}_full_ft"
WANDB_PROJECT = "huggingface" # project name for normal full fts
LOG_STEPS = 1 if TRAIN_FILE == "tiny_monolingual_warao_bible.txt" else 100
WARMUP_STEPS = 0

# load dataset
# load monolingual data
dataset = load_dataset("text", data_files={"train": TRAIN_FILE})
print(dataset)

if torch.cuda.is_available():
    torch.cuda.empty_cache()

# load tokenizer
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)

print(f'Vocab size: {tokenizer.vocab_size}') # 128004
# print(f'Config: {tokenizer.config}')
# print(f'Special tokens: {tokenizer.special_tokens_map}')
print(f'Context length: {tokenizer.model_max_length}') # 1024

print(tokenizer.pad_token)
print(tokenizer.eos_token)

if tokenizer.model_max_length > 1024:
  tokenizer.model_max_length = 1024
tokenizer.pad_token = tokenizer.eos_token

# fertility rate for single example:
example = dataset['train'][0]
num_words = len(example['text'].split())
print(f'Number of words: {num_words}')

input_ids = tokenizer(example['text'])['input_ids']
print(f'Number of tokens: {len(input_ids)}')

# high score means each word in sentence is split into lots of tokens
print(f'Fertility rate: {len(input_ids) / num_words }')

tokens = tokenizer.convert_ids_to_tokens(input_ids)
print(example['text'])
print(tokens)
print(tokenizer.decode(input_ids))

# if you have long sentences and v short tokens,
# we can quickly pass the context length

# #approximate FULL fertility score
# def num_words(sentence):
#   return len(sentence.split())

# dataset['train'].map(lambda sentence: num_words(sentence['text']))

def tokenize(example):
  toks = tokenizer(example['text'], padding=False, truncation=True)
  return toks

tokenized_ids = dataset.map(tokenize, batched=True, num_proc=12, remove_columns=['text'])
tokenized_ids

# """
# Packing sequence:
# a data-efficiency trick where shorter text samples are
# concatenated together into a single training sequence so that the model's context
# window is utilized as fully as possible
#  """

# def concatenate_and_chunk(examples):
#   pass

# ds_chunked = dataset.map(concatenate_and_chunk,
#                     batch_size=1000,
#                     batched=True,
#                     num_proc=12,
#                     remove_columns=['text']
#                     )

# ds_chunked.save_to_disk('monolingual_warao_chunked')

# ds_chunked = load_from_disk('monolingual_warao_chunked')
# print(ds_chunked)

# data collator
data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=MODEL_NAME)

# load model
model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME) #, pad_token_id=tokenizer.eos_token_id)

configuration = model.config
print(configuration)

print(model)

num_parameters = 0 # number of LEARNABLE parameters ONLY
for param in model.parameters():
  num_parameters += param.numel()
print(f'Number of parameters: {num_parameters/10**6:.2f} M')

# compute memory requirements
print(model.dtype)

mem_in_gb = num_parameters * 4 / 1e9
print(f'Memory requirements (learnable params only): {mem_in_gb:.2f} GB')

# just for loading model -- need more for training (for storing gradients + GPU kernels need mem too based on GPU type)
print(f'Memory requirements (all params): {model.get_memory_footprint()/1e9} GB')

param_model = num_parameters * 4/1e9
adam_opt = 3*param_model # for storing moments
kernel = 1
bs = BATCH_SIZE
print(f'Total Memory requirements per batch: {(param_model+adam_opt+kernel) * bs} GB')


# we can use gradient accumulation strategy but this increases training time
# we can use PEFT to reduce memory requirements -- given our limited compute

def prepare_batch(batch_texts, tokenizer, src_lang, tgt_lang, model_name, max_length=128):

    if "m2m100" in model_name:
        tokenizer.src_lang = src_lang
        inputs = tokenizer(batch_texts, return_tensors="pt", padding=True,
                          truncation=True, max_length=max_length).to(device)
        forced_bos_token_id = tokenizer.get_lang_id(tgt_lang)

    elif "mbart" in model_name:
        tokenizer.src_lang = src_lang
        inputs = tokenizer(batch_texts, return_tensors="pt", padding=True,
                          truncation=True, max_length=max_length).to(device)
        forced_bos_token_id = tokenizer.lang_code_to_id[tgt_lang]

    elif "nllb" in model_name:
        tokenizer.src_lang = src_lang
        inputs = tokenizer(batch_texts, return_tensors="pt", padding=True,
                          truncation=True, max_length=max_length).to(device)
        forced_bos_token_id = tokenizer.convert_tokens_to_ids(tgt_lang)

    elif "mt5" in model_name or "byt5" in model_name:
        # T5 models use task prefix instead of language codes
        batch_with_prefix = [f"translate {src_lang} to {tgt_lang}: {text}"
                            for text in batch_texts]
        inputs = tokenizer(batch_with_prefix, return_tensors="pt", padding=True,
                          truncation=True, max_length=max_length).to(device)
        forced_bos_token_id = None

    else:
        raise ValueError(f"Unknown model type: {model_name}")

    return inputs, forced_bos_token_id

device = "cuda" if torch.cuda.is_available() else "cpu"
# lets see how model perform out of the box on warao to spanish translation
ex = dataset['train'][5]
inputs, forced_bos_token_id = prepare_batch(ex, tokenizer, SOURCE_CODE, TARGET_CODE, MODEL_NAME, max_length=MAX_LEN)[0]
if forced_bos_token_id is not None:
    outputs = model.generate(
        **inputs,
        forced_bos_token_id=forced_bos_token_id,
        max_length=MAX_LEN,
        num_beams=NUM_BEAMS
    )
else:
    outputs = model.generate(
        **inputs,
        max_length=MAX_LEN,
        num_beams=NUM_BEAMS
    )
print(tokenizer.batch_decode(outputs, skip_special_tokens=True))

# config = LoRAConfig(
#     r = 8, #rank of matrices
#     lora_alpha = 16,
#     lora_dropout = 0.05,
#     bias = "none",
#     task_type = "SEQ_2_SEQ_LM",
#     #target_modules=["q_proj", "v_proj"]
# )