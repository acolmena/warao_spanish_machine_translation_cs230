{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5092cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64144928",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_two_column_text(pdf_path, start_pg_num=0, end_pg_num=None, print_every_n_pages=50):\n",
    "    \"\"\"\n",
    "    Extract text from a two-column PDF, ignoring middle column.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path: Path to PDF file\n",
    "        start_pg_num: Page from which to start extracting (0-indexed)\n",
    "    \n",
    "    Returns:\n",
    "        Extracted text as string\n",
    "    \"\"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        pages = pdf.pages[start_pg_num:end_pg_num]\n",
    "        \n",
    "        text_output = \"\"\n",
    "        for page in pages:\n",
    "            # Get page dimensions\n",
    "            page_width = page.width\n",
    "            page_height = page.height\n",
    "            \n",
    "            # Define column boundaries (adjust these based on your PDF)\n",
    "            left_column = {\n",
    "                'x0': 0,\n",
    "                'top': 0,\n",
    "                'x1': page_width * 0.45,  # Left 45% of page\n",
    "                'bottom': page_height\n",
    "            }\n",
    "            \n",
    "            right_column = {\n",
    "                'x0': page_width * 0.55,  # Right 45% of page (skip middle 10%)\n",
    "                'top': 0,\n",
    "                'x1': page_width,\n",
    "                'bottom': page_height\n",
    "            }\n",
    "            \n",
    "            # Extract text from each column\n",
    "            left_text = page.within_bbox(\n",
    "                (left_column['x0'], left_column['top'], \n",
    "                left_column['x1'], left_column['bottom'])\n",
    "            ).extract_text()\n",
    "            \n",
    "            right_text = page.within_bbox(\n",
    "                (right_column['x0'], right_column['top'], \n",
    "                right_column['x1'], right_column['bottom'])\n",
    "            ).extract_text()\n",
    "            \n",
    "            # Combine columns\n",
    "            text_output += f\"{left_text}\\n\\n{right_text}\"\n",
    "\n",
    "            if page.page_number % print_every_n_pages == 0:\n",
    "                print(f\"up to page {page.page_number} done\")\n",
    "        \n",
    "        return text_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "049d359a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "up to page 20 done\n",
      "up to page 40 done\n",
      "up to page 60 done\n",
      "up to page 80 done\n",
      "up to page 100 done\n",
      "up to page 120 done\n",
      "up to page 140 done\n",
      "up to page 160 done\n",
      "up to page 180 done\n",
      "up to page 200 done\n",
      "up to page 220 done\n",
      "up to page 240 done\n",
      "up to page 260 done\n",
      "up to page 280 done\n",
      "up to page 300 done\n",
      "up to page 320 done\n",
      "up to page 340 done\n",
      "up to page 360 done\n",
      "up to page 380 done\n",
      "up to page 400 done\n",
      "up to page 420 done\n",
      "up to page 440 done\n",
      "up to page 460 done\n",
      "up to page 480 done\n",
      "up to page 500 done\n",
      "up to page 520 done\n",
      "up to page 540 done\n",
      "up to page 560 done\n",
      "up to page 580 done\n",
      "up to page 600 done\n",
      "up to page 620 done\n",
      "up to page 640 done\n",
      "up to page 660 done\n",
      "up to page 680 done\n",
      "up to page 700 done\n",
      "up to page 720 done\n"
     ]
    }
   ],
   "source": [
    "START_PG_NUM = 10\n",
    "END_PG_NUM = 731\n",
    "PRINT_EVERY_N_PAGES = 100\n",
    "\n",
    "cwd = os.path.abspath(os.getcwd())\n",
    "os.chdir(cwd)  # changes cwd to script's directory\n",
    "data_dirname = \"data\"\n",
    "data_dir = os.path.join(cwd, data_dirname)\n",
    "# create data directory if it doesn't exist\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)\n",
    "input_filename = \"Warao_Bible_cropped.pdf\"\n",
    "input_path = os.path.join(data_dir, input_filename)\n",
    "text = extract_two_column_text(\n",
    "                                input_path, \n",
    "                                start_pg_num=START_PG_NUM, \n",
    "                                end_pg_num=END_PG_NUM, \n",
    "                                print_every_n_pages=PRINT_EVERY_N_PAGES\n",
    "                                )\n",
    "# print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "552ee661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split text into sentences\n",
    "text_array = np.array(re.split(r'[?!.]+', text))  # split sentences by any end punctuation (?, ., or !)\n",
    "\n",
    "threshold = 1\n",
    "filtered = []\n",
    "for sentence in text_array:\n",
    "    # sentence = re.sub(r'[^a-zA-Z\\s]', '', sentence)  # remove special characters\n",
    "    sentence = re.sub(r'[\\n]', '', sentence)  # remove newline characters\n",
    "    if len(sentence.split()) > threshold:\n",
    "        sentence = sentence.strip()  # remove leading and trailing whitespace\n",
    "        sentence = sentence.lstrip('0123456789')\n",
    "        filtered.append(sentence)\n",
    "# display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3a26e445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of duplicate sentences: 831\n",
      "number of sentences: 26399\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>warao_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2021</th>\n",
       "      <td>Isaac Jacob yake rajatane abayaja akajo tabu e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2022</th>\n",
       "      <td>Ribane:Canaán anobo tomo tatu kawa-namo tira n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023</th>\n",
       "      <td>Yake raja tane abakore Esaúnaminae yama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2024</th>\n",
       "      <td>Jacob arimaarani tane aribu eku nona-yaja Pada...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2025</th>\n",
       "      <td>Aribu eku nona koreEsaú naminae yama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2026</th>\n",
       "      <td>Canaánanobo tomo tatu jakutai Isaacaobo jona e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2027</th>\n",
       "      <td>Naminai tane Ismael tatanaruae yama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2028</th>\n",
       "      <td>Ismael jakutai taiAbraham auka</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2029</th>\n",
       "      <td>Esaú ori ata tusiatekoro tane Ismael auka tira...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2030</th>\n",
       "      <td>Mahalatjakutai tai Nebaiot arakoi sanu-kema</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         warao_sentence\n",
       "2021  Isaac Jacob yake rajatane abayaja akajo tabu e...\n",
       "2022  Ribane:Canaán anobo tomo tatu kawa-namo tira n...\n",
       "2023            Yake raja tane abakore Esaúnaminae yama\n",
       "2024  Jacob arimaarani tane aribu eku nona-yaja Pada...\n",
       "2025               Aribu eku nona koreEsaú naminae yama\n",
       "2026  Canaánanobo tomo tatu jakutai Isaacaobo jona e...\n",
       "2027                Naminai tane Ismael tatanaruae yama\n",
       "2028                     Ismael jakutai taiAbraham auka\n",
       "2029  Esaú ori ata tusiatekoro tane Ismael auka tira...\n",
       "2030        Mahalatjakutai tai Nebaiot arakoi sanu-kema"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = pd.DataFrame(filtered, columns=[\"warao_sentence\"])\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "print(f\"number of duplicate sentences: {len(filtered) - len(df)}\")\n",
    "print(f\"number of sentences: {len(df)}\")\n",
    "\n",
    "display(df[2000:2010])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e995fafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(os.path.join(data_dir, \"monolingual_warao_sentences_bible.csv\"), index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs230-warao",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
